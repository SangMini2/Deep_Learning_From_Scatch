{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pfY0WY06Og1QgVB7Eh1gtDwArSMMx_MF",
      "authorship_tag": "ABX9TyNymTp1QWekytagg2xAN8ac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangMini2/Deep_Learning_From_Scatch/blob/main/11_23_STUDY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jfDQlHbIoY7",
        "outputId": "5754f528-2f51-4a1d-9304-e1aae4590939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "T,H = 5,4\n",
        "hs = np.random.randn(T,H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05 , 0.02])\n",
        "\n",
        "ar = a.reshape(5,1).repeat(4, axis = 1)\n",
        "print(ar.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpugtn3LJwLS",
        "outputId": "7082b22a-1d85-435e-ed71-ababa55e3080"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.8  0.8  0.8  0.8 ]\n",
            " [0.1  0.1  0.1  0.1 ]\n",
            " [0.03 0.03 0.03 0.03]\n",
            " [0.05 0.05 0.05 0.05]\n",
            " [0.02 0.02 0.02 0.02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이해를 돕기위해 돌려본 코드\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05 , 0.02])\n",
        "\n",
        "ab = a.reshape(5,1)\n",
        "ab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRoLW1L5J7eL",
        "outputId": "0618d7e1-6622-479a-a699-1c23a1af7f9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8 ],\n",
              "       [0.1 ],\n",
              "       [0.03],\n",
              "       [0.05],\n",
              "       [0.02]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lo3kD_6KYdJ",
        "outputId": "c3757332-03a3-4efa-d0ed-4aae2b165649"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hs와 a를 계산할 수 있게 하기 위해서 (5,4)shape인 ar을 만들어 주어 계산합니다."
      ],
      "metadata": {
        "id": "G9E3hi4XKbVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = hs * ar\n",
        "print(t)\n",
        "print(t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FE3Q_TrJC5h",
        "outputId": "3b9594f4-c443-4770-d4c5-dff2a5d03a72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.29645059  0.77945308 -0.20532613  0.21355725]\n",
            " [-0.01501181  0.08486962  0.10491732  0.08409903]\n",
            " [-0.01262321  0.00786522  0.06345055  0.00167015]\n",
            " [-0.05114915  0.02830944  0.01098248 -0.05419054]\n",
            " [-0.00840586  0.02440166  0.01971432 -0.01291718]]\n",
            "(5, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.sum(t, axis = 0) # axis=0 의 의미는 세로로 다 더해준다는 의미입니다.\n",
        "print(c)\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvpP08HcJOsE",
        "outputId": "3a3fba7f-813a-4d98-9b71-ca039e3633f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.20926055  0.92489902 -0.00626146  0.23221871]\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니배치 처리용 가중합 구현"
      ],
      "metadata": {
        "id": "WDpCP8WRLOvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사실 3차원부터 어디에 해당하는 것인지 이해가 잘 안됩니다.."
      ],
      "metadata": {
        "id": "dbWbXW28Q6cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis = 2)\n"
      ],
      "metadata": {
        "id": "64Z0RQh3Jr5f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRywtzM0LS_c",
        "outputId": "908a07db-a3df-4da3-cf11-961f535f8eda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.98644345, -0.98644345, -0.98644345, -0.98644345],\n",
              "        [-0.15947935, -0.15947935, -0.15947935, -0.15947935],\n",
              "        [-0.82399205, -0.82399205, -0.82399205, -0.82399205],\n",
              "        [ 2.28430434,  2.28430434,  2.28430434,  2.28430434],\n",
              "        [-2.09273095, -2.09273095, -2.09273095, -2.09273095]],\n",
              "\n",
              "       [[ 0.34553061,  0.34553061,  0.34553061,  0.34553061],\n",
              "        [ 0.1945381 ,  0.1945381 ,  0.1945381 ,  0.1945381 ],\n",
              "        [-0.19007739, -0.19007739, -0.19007739, -0.19007739],\n",
              "        [-0.11370091, -0.11370091, -0.11370091, -0.11370091],\n",
              "        [-2.08842488, -2.08842488, -2.08842488, -2.08842488]],\n",
              "\n",
              "       [[-0.04332303, -0.04332303, -0.04332303, -0.04332303],\n",
              "        [-0.16773744, -0.16773744, -0.16773744, -0.16773744],\n",
              "        [ 0.09233852,  0.09233852,  0.09233852,  0.09233852],\n",
              "        [-0.89673473, -0.89673473, -0.89673473, -0.89673473],\n",
              "        [-1.00565411, -1.00565411, -1.00565411, -1.00565411]],\n",
              "\n",
              "       [[-1.01486857, -1.01486857, -1.01486857, -1.01486857],\n",
              "        [ 1.6248408 ,  1.6248408 ,  1.6248408 ,  1.6248408 ],\n",
              "        [ 1.37017348,  1.37017348,  1.37017348,  1.37017348],\n",
              "        [-0.84518962, -0.84518962, -0.84518962, -0.84518962],\n",
              "        [ 0.13628139,  0.13628139,  0.13628139,  0.13628139]],\n",
              "\n",
              "       [[-0.19543659, -0.19543659, -0.19543659, -0.19543659],\n",
              "        [ 1.4240057 ,  1.4240057 ,  1.4240057 ,  1.4240057 ],\n",
              "        [-0.19321157, -0.19321157, -0.19321157, -0.19321157],\n",
              "        [ 0.98833943,  0.98833943,  0.98833943,  0.98833943],\n",
              "        [ 1.53513349,  1.53513349,  1.53513349,  1.53513349]],\n",
              "\n",
              "       [[-2.79189088, -2.79189088, -2.79189088, -2.79189088],\n",
              "        [-0.11641264, -0.11641264, -0.11641264, -0.11641264],\n",
              "        [ 2.55952977,  2.55952977,  2.55952977,  2.55952977],\n",
              "        [-0.49240497, -0.49240497, -0.49240497, -0.49240497],\n",
              "        [-0.72314761, -0.72314761, -0.72314761, -0.72314761]],\n",
              "\n",
              "       [[-0.01403267, -0.01403267, -0.01403267, -0.01403267],\n",
              "        [-0.00579664, -0.00579664, -0.00579664, -0.00579664],\n",
              "        [ 1.26815201,  1.26815201,  1.26815201,  1.26815201],\n",
              "        [-0.81932464, -0.81932464, -0.81932464, -0.81932464],\n",
              "        [ 0.70513592,  0.70513592,  0.70513592,  0.70513592]],\n",
              "\n",
              "       [[ 1.05329686,  1.05329686,  1.05329686,  1.05329686],\n",
              "        [ 0.44296253,  0.44296253,  0.44296253,  0.44296253],\n",
              "        [-0.70157082, -0.70157082, -0.70157082, -0.70157082],\n",
              "        [ 0.31290879,  0.31290879,  0.31290879,  0.31290879],\n",
              "        [-2.42187829, -2.42187829, -2.42187829, -2.42187829]],\n",
              "\n",
              "       [[ 0.30755044,  0.30755044,  0.30755044,  0.30755044],\n",
              "        [-1.95875728, -1.95875728, -1.95875728, -1.95875728],\n",
              "        [ 0.84854482,  0.84854482,  0.84854482,  0.84854482],\n",
              "        [ 0.33426519,  0.33426519,  0.33426519,  0.33426519],\n",
              "        [-0.49535071, -0.49535071, -0.49535071, -0.49535071]],\n",
              "\n",
              "       [[-0.12257109, -0.12257109, -0.12257109, -0.12257109],\n",
              "        [-1.5965754 , -1.5965754 , -1.5965754 , -1.5965754 ],\n",
              "        [-0.78542467, -0.78542467, -0.78542467, -0.78542467],\n",
              "        [-1.78772076, -1.78772076, -1.78772076, -1.78772076],\n",
              "        [-0.72493658, -0.72493658, -0.72493658, -0.72493658]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그림 8-11을 보면, 지금까지의 과정들이 계산과 함께 나와있습니다."
      ],
      "metadata": {
        "id": "u-3NGErtMD3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "역전파를 구해보겠습니다.\n",
        "\n",
        "'sum'과 'repeat'은 서로의 역전파입니다.\n",
        "\n",
        "다시 얘기하자면 'sum'의 역전파는 'repeat'이고, 'repeat'의 역전파는 'sum'입니다."
      ],
      "metadata": {
        "id": "IZDI1uCONvYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightSum :\n",
        "  def __init__(self) :\n",
        "    self.params, self.grads = [], []\n",
        "    self.cache = None\n",
        "  \n",
        "  def forward(self, hs, a) :\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "    t = hs * ar\n",
        "    c = np.sum(t, axis = 1)\n",
        "\n",
        "    self.cache = (hs, ar)\n",
        "    return c\n",
        "  \n",
        "  def backward(self, dc) :\n",
        "    hs, ar = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    dt = dc.reshape(N, 1, H).repeat(T, axis=1) #그림 8-11에서의 마지막 sum에서의 역전파\n",
        "    dar = dt * hs\n",
        "    dhs = dt * ar\n",
        "    da = np.sum(dar, axis = 2) # repeat의 역전파\n",
        "\n",
        "    return dhs, da"
      ],
      "metadata": {
        "id": "0D604x61LyyN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다시 ppt로 돌아가겠습니다."
      ],
      "metadata": {
        "id": "1D_1l-i8xyp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "Tk57wZD7Ly02"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/deep-learning-from-scratch-2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpQmCj5I0iuM",
        "outputId": "8df27dd5-2edc-40be-832b-4cf9016d9e70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/deep-learning-from-scratch-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from common.layers import Softmax\n"
      ],
      "metadata": {
        "id": "NY1u8ajcLy3L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N,1,H).repeat(T, axis = 1)\n",
        "\n",
        "t = hs * hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t, axis = 2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s) #가중치\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em7ABi481kV7",
        "outputId": "566cabaa-aafa-421f-97d5-2584a8bd47b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWeight :\n",
        "  def __init__(self) :\n",
        "    self.params, self.grads = [], []\n",
        "    self.softmax = Softmax()\n",
        "    self.cache = None\n",
        "  \n",
        "  def forward(self, hs, a) :\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    hr = a.reshape(N, 1, H).repeat(T, axis = 1)\n",
        "    t = hs * hr\n",
        "    s = np.sum(t, axis = 2)\n",
        "    a = self.softmax.forward(s)\n",
        "\n",
        "    self.cache = (hs, hr)\n",
        "    return a\n",
        "  \n",
        "  def backward(self, da) :\n",
        "    hs, hr = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ds = self.softmax.backward(da)\n",
        "    dt = ds.reshape(N, T, 1).repeat(H, axis = 2)\n",
        "    dhs = dt * hr\n",
        "    dhr = dt * hs\n",
        "    dh = np.sum(dhr, axis = 1)\n",
        "\n",
        "    return dhs, dh"
      ],
      "metadata": {
        "id": "JS3UMQao20gX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention :\n",
        "  def __init__(self) :\n",
        "    self.params, self.grads = [], []\n",
        "    self.attention_weight_layer = AttentionWeight()\n",
        "    self.weight_sum_layer = WeightSum()\n",
        "    self.attention_weight = None\n",
        "  \n",
        "  def forward(self, hs, h) :\n",
        "    a = self.attention_weight_layer.forward(hs, h)\n",
        "    out = self.weight_sum_layer.forward(hs, a)\n",
        "    self.attention_weight = a #가중치를 a로 두는,,\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout) :\n",
        "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "    dhs = dhs0 + dhs1\n",
        "    return dhs, dh"
      ],
      "metadata": {
        "id": "_9Bx2KXm20j1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention 계층을 우리는 LSTM계층과 Affine계층 사이에 삽입하면 됩니다.\n",
        "\n",
        "(그림 8-18 참고)\n",
        "\n",
        "그림 8-18을 보면 LSTM 계층의 은닉 상태 벡터를 Affine 계층에 입력합니다. 이는 앞 장에서 본 Decoder의 개선으로부터 자연스럽게 확장된 것으로 볼 수 있습니다."
      ],
      "metadata": {
        "id": "KJ7-jK9WMmSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weights = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
        "            self.layers.append(layer)\n",
        "            self.attention_weights.append(layer.attention_weight)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :])\n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:,t,:] = dh\n",
        "\n",
        "        return dhs_enc, "
      ],
      "metadata": {
        "id": "MnxyDdV9PT5d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "\n",
        "class AttentionEncoder(Encoder) :\n",
        "  def forward(self, xs) :\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)\n",
        "    return xs\n",
        "    \n",
        "  def backward(self, dhs) :\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout\n",
        "\n"
      ],
      "metadata": {
        "id": "UIASC3RyRHuB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common.time_layers import TimeEmbedding, TimeLSTM, TimeAffine\n",
        "\n",
        "# 코드를 이해 못했습니다.. 아직..\n",
        "\n",
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled"
      ],
      "metadata": {
        "id": "tQlUnsBFSyIV"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}